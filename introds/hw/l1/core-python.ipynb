{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerardo Caracas Uribe<br>\n",
    "Assigment: L1-Core Python<br>\n",
    "Date: August 23th. 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"     Hello, world!   \\t\\t\\n\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', 'world!']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hello,  world!\".split() # Two spaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello,', '', 'world!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Hello,  world!\".split(\" \") # Two spaces!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['www', 'networkssciencelab', 'com']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"www.networkssciencelab.com\".split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alpha, bravo, charlie, delta'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\", \".join([\"alpha\", \"bravo\", \"charlie\", \"delta\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1-617-305-1985'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"-\".join(\"1.617.305.1985\".split(\".\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This string has many spaces'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(\"This string \\n\\r  has     many \\t\\tspaces\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"www.networksciencelab.com\".find(\".com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"www.networksciencelab.com\".count(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'alpha', 1: 'bravo', 2: 'charlie', 3: 'delta'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = [\"alpha\",\"bravo\",\"charlie\",\"delta\"]\n",
    "dict(enumerate(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 'alpha', 'b': 'bravo', 'c': 'charlie', 'd': 'delta'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kseq=\"abcd\" # A string is a sequence, too\n",
    "vseq = [\"alpha\", \"bravo\", \"charlie\", \"delta\"]\n",
    "dict(zip(kseq,vseq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('man', 1), ('plan', 1), ('canal', 1), ('panama', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "phrase = \"a man a plan a canal panama\"\n",
    "cntr = Counter(phrase.split())\n",
    "cntr.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 3, 'man': 1, 'plan': 1, 'canal': 1, 'panama': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cntDict = dict(cntr.most_common())\n",
    "cntDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cntDict['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "try:\n",
    "    with urllib.request.urlopen(\"http://www.networksciencelab.com\") as doc:\n",
    "        html = doc.read()\n",
    "        # if reading was successful, the connection is closed automatically\n",
    "except:\n",
    "    print(\"Could not open %s\" %doc, file=sys.err)\n",
    "    # Do note pretend that the document has been read!\n",
    "    # Execute an error handler here\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParseResult(scheme='http', netloc='networksciencelab.com', path='/index.html', params='param', query='foo=baf', fragment='content')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.parse\n",
    "URL=\"http://networksciencelab.com/index.html;param?foo=baf#content\"\n",
    "urllib.parse.urlparse(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', '', 'world', '']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split(r\"\\W\", \"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'world', '']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Combine all adjacent non letters\n",
    "re.split(r\"\\W+\",\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = re.match(r\"\\d+\", \"067 Starts with a number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'067'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mo.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function re.match(pattern, string, flags=0)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match(r\"\\d+\", \"Does not start with a number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(8, 11), match='Has'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r\"[a-z]+\", \"0010010 Has at lest one 010 letter 0010010\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(9, 11), match='as'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Casse sensitive Version\n",
    "re.search(r\"[a-z]+\", \"0010010 Has at least one 010 letter 0010010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Has', 'at', 'least', 'one', 'letter']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"[a-z]+\", \"0010010 Has at least one 010 letter 0010010\", re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0010010 [...] [...] [...] [...] 010 [...] 0010010'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"[a-z]+\", \"[...]\", \"0010010 has at least one 010 letter 0010010\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test.txt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "glob.glob(\"*.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User, fill in the URL variable below, the url address you wnat to process.\n",
    "URL=\"http://www.gutenberg.org/files/2701/2701-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now read the page, with exception handling\n",
    "import urllib.request\n",
    "try:\n",
    "    with urllib.request.urlopen(URL) as doc:\n",
    "        html = doc.read()\n",
    "        # if reading was successful, the connection is closed automatically\n",
    "except:\n",
    "    print(\"Could not open %s\" %doc, file=sys.err)\n",
    "    # Do note pretend that the document has been read!\n",
    "    # Execute an error handler here\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "html variable is a byte type variable, we need to convert it into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = html.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now le'ts split the words case insensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "data = re.split(r\"\\W+\", html,  flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get the most common words and list the top 10 without using a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 13972),\n",
       " ('of', 6699),\n",
       " ('and', 6144),\n",
       " ('a', 4648),\n",
       " ('to', 4635),\n",
       " ('in', 3997),\n",
       " ('that', 2994),\n",
       " ('his', 2472),\n",
       " ('it', 2222),\n",
       " ('I', 2120)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "cntr = Counter(data)\n",
    "result=cntr.most_common()\n",
    "result[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save our output for future usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"myData.pickle\",\"wb\") as oFile:\n",
    "    pickle.dump(data, oFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's doublecheck the object has been sotored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object of type = <class 'list'>\n",
      "Length of object =  222665\n",
      "Sample of object ['', 'The', 'Project', 'Gutenberg', 'EBook']\n"
     ]
    }
   ],
   "source": [
    "with open(\"myData.pickle\",\"rb\") as iFile:\n",
    "    object = pickle.load(iFile)\n",
    "print(\"Object of type =\",type(object))\n",
    "print(\"Length of object = \", len(object))\n",
    "print(\"Sample of object\", object[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed packages\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path_to_files = path where the files to be processed reside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_files=\"files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's list the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['files/tomsawer.txt',\n",
       " 'files/thespectatorvolume1.txt',\n",
       " 'files/thestoryofthezulucampaign.txt']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = glob.glob(path_to_files+\"*.txt\")\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate here through each one of the files making the following operations per file:\n",
    "* Read the data as a whole block\n",
    "* Split the data as an array or words, case insensitive. And store it in a list\n",
    "* We remove the white spaces, and empty strings from the list\n",
    "* We are not counting frequency of words, thus we will remove duplicates, by converting it in a set, and then convert it back into a list\n",
    "* Define a temporarly dictionary where we add {filename:series of words}\n",
    "* Finally we append the result into our final dataframe, which will contain all the words in all the files.\n",
    "\n",
    "*** Note that in all this excercise we loop only through the file list but never through the contents of the file. We are using the pandas capability of processing data without a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finalDataframe = pd.DataFrame()\n",
    "for file in files:\n",
    "    with open(file, mode=\"r\") as f:\n",
    "        data = f.read()\n",
    "        # Split data into array of words, non case sensitive\n",
    "        word = re.split(r\"\\W+\", data,  flags=re.IGNORECASE)\n",
    "        # Remove withe spaces and empty strings\n",
    "        cleanWords = [line for line in [l.strip() for l in word] if line]\n",
    "        # Remove duplicates, we don't want them\n",
    "        words = list(set(cleanWords))\n",
    "        # Add data into dictionary\n",
    "        dictionary = {\"filename\":file, \"values\":pd.Series(words)}\n",
    "        finalDataframe = finalDataframe.append(pd.DataFrame(dictionary))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the result data, here is the printout if the head of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>files/tomsawer.txt</td>\n",
       "      <td>jib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>files/tomsawer.txt</td>\n",
       "      <td>waverer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>files/tomsawer.txt</td>\n",
       "      <td>perilous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>files/tomsawer.txt</td>\n",
       "      <td>killed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>files/tomsawer.txt</td>\n",
       "      <td>furnish</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             filename    values\n",
       "0  files/tomsawer.txt       jib\n",
       "1  files/tomsawer.txt   waverer\n",
       "2  files/tomsawer.txt  perilous\n",
       "3  files/tomsawer.txt    killed\n",
       "4  files/tomsawer.txt   furnish"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the tail, note that the filename is different, in the filename column we will add all the file names sequentially. Thus could potentially have a duplication of words. That is ok, we will handle that soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10420</th>\n",
       "      <td>files/thestoryofthezulucampaign.txt</td>\n",
       "      <td>apprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10421</th>\n",
       "      <td>files/thestoryofthezulucampaign.txt</td>\n",
       "      <td>defenders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10422</th>\n",
       "      <td>files/thestoryofthezulucampaign.txt</td>\n",
       "      <td>occurred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10423</th>\n",
       "      <td>files/thestoryofthezulucampaign.txt</td>\n",
       "      <td>rest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10424</th>\n",
       "      <td>files/thestoryofthezulucampaign.txt</td>\n",
       "      <td>Ocean</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  filename     values\n",
       "10420  files/thestoryofthezulucampaign.txt   apprised\n",
       "10421  files/thestoryofthezulucampaign.txt  defenders\n",
       "10422  files/thestoryofthezulucampaign.txt   occurred\n",
       "10423  files/thestoryofthezulucampaign.txt       rest\n",
       "10424  files/thestoryofthezulucampaign.txt      Ocean"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDataframe.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we will use the pandas crosstab function. The main reason we choose this, is because it will generate a table with all the words with no duplication (important) and it will create a matrix to make a relationship between words and on which files a given word is included into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossTabMatrix = pd.crosstab(finalDataframe['values'], finalDataframe['filename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of the crossTab Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>filename</th>\n",
       "      <th>files/thespectatorvolume1.txt</th>\n",
       "      <th>files/thestoryofthezulucampaign.txt</th>\n",
       "      <th>files/tomsawer.txt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>values</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ære</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ætas</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>æternaque</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>è</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>échelon_</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "filename   files/thespectatorvolume1.txt  files/thestoryofthezulucampaign.txt  \\\n",
       "values                                                                          \n",
       "ære                                    1                                    0   \n",
       "ætas                                   1                                    0   \n",
       "æternaque                              1                                    0   \n",
       "è                                      1                                    0   \n",
       "échelon_                               0                                    1   \n",
       "\n",
       "filename   files/tomsawer.txt  \n",
       "values                         \n",
       "ære                         0  \n",
       "ætas                        0  \n",
       "æternaque                   0  \n",
       "è                           0  \n",
       "échelon_                    0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossTabMatrix.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will iterate for the second time through the list of filenames, so we can replace the [1,0] values in the crossTab matrix, and replace it with the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in list(crossTabMatrix.columns.values):\n",
    "    crossTabMatrix[file].replace([0,1],['',file],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of our matrix with filenames rather than [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>filename</th>\n",
       "      <th>files/thespectatorvolume1.txt</th>\n",
       "      <th>files/thestoryofthezulucampaign.txt</th>\n",
       "      <th>files/tomsawer.txt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>values</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ære</th>\n",
       "      <td>files/thespectatorvolume1.txt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ætas</th>\n",
       "      <td>files/thespectatorvolume1.txt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>æternaque</th>\n",
       "      <td>files/thespectatorvolume1.txt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>è</th>\n",
       "      <td>files/thespectatorvolume1.txt</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>échelon_</th>\n",
       "      <td></td>\n",
       "      <td>files/thestoryofthezulucampaign.txt</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "filename   files/thespectatorvolume1.txt  files/thestoryofthezulucampaign.txt  \\\n",
       "values                                                                          \n",
       "ære        files/thespectatorvolume1.txt                                        \n",
       "ætas       files/thespectatorvolume1.txt                                        \n",
       "æternaque  files/thespectatorvolume1.txt                                        \n",
       "è          files/thespectatorvolume1.txt                                        \n",
       "échelon_                                  files/thestoryofthezulucampaign.txt   \n",
       "\n",
       "filename  files/tomsawer.txt  \n",
       "values                        \n",
       "ære                           \n",
       "ætas                          \n",
       "æternaque                     \n",
       "è                             \n",
       "échelon_                      "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crossTabMatrix.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32453, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimension of the matrix, as we can see in this example, we have 3 columns. One per file name\n",
    "crossTabMatrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following step, we will convert the values of each row, into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "setOfLists=crossTabMatrix.apply(lambda x: x.tolist(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of our set of lists. Note that we are getting closer to our final solution. We wet the word, and then a list of files that include that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "values\n",
       "ære                [files/thespectatorvolume1.txt, , ]\n",
       "ætas               [files/thespectatorvolume1.txt, , ]\n",
       "æternaque          [files/thespectatorvolume1.txt, , ]\n",
       "è                  [files/thespectatorvolume1.txt, , ]\n",
       "échelon_     [, files/thestoryofthezulucampaign.txt, ]\n",
       "dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setOfLists.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we convert setOfLists into a dictionary as a final solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDict = setOfLists.to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample of our final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 ] =  ['', '', 'files/tomsawer.txt']\n",
      "[ 000 ] =  ['files/thespectatorvolume1.txt', 'files/thestoryofthezulucampaign.txt', 'files/tomsawer.txt']\n",
      "[ 000_l ] =  ['', 'files/thestoryofthezulucampaign.txt', '']\n"
     ]
    }
   ],
   "source": [
    "iterator = iter(finalDict.items())\n",
    "for i in range(3):\n",
    "    k,v = next(iterator)\n",
    "    print(\"[\",k,\"] = \",v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
